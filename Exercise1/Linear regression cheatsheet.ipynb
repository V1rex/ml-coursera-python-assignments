{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4259c255",
   "metadata": {},
   "source": [
    "# Introduction \n",
    "This is a summary(or maybe a cheatsheet) from week 2\n",
    "\n",
    "**we will start by importing some libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "919d48c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# used for manipulating directory paths\n",
    "import os\n",
    "\n",
    "# Scientific and vector computation for python\n",
    "import numpy as np\n",
    "\n",
    "# Plotting library\n",
    "from matplotlib import pyplot\n",
    "from mpl_toolkits.mplot3d import Axes3D  # needed to plot 3-D surfaces"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cafdee15",
   "metadata": {},
   "source": [
    "# Linear regression with mutiple variables "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c44c9f1",
   "metadata": {},
   "source": [
    "## Loading data with mutiple features "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "570ffd49",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.loadtxt(os.path.join('Data', 'ex1data2.txt'), delimiter=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46a3f464",
   "metadata": {},
   "source": [
    "We will then put the features in X(which are in this case: the size of the house and the number of bedrooms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f32ae572",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data[:, :2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e5f6fcf",
   "metadata": {},
   "source": [
    "and then the expected outcome y (which is in this case : the price of the house in y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2d1890b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = data[:, 2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "294c39df",
   "metadata": {},
   "source": [
    "We will see now what the data looks like "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2c26d989",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  X[:,0] X[:, 1]         y\n",
      "--------------------------\n",
      "    2104       3    399900\n",
      "    1600       3    329900\n",
      "    2400       3    369000\n",
      "    1416       2    232000\n",
      "    3000       4    539900\n",
      "    1985       4    299900\n",
      "    1534       3    314900\n",
      "    1427       3    198999\n",
      "    1380       3    212000\n",
      "    1494       3    242500\n"
     ]
    }
   ],
   "source": [
    "# m represents the number of training examples \n",
    "m = y.size\n",
    "\n",
    "# print out some data points\n",
    "print('{:>8s}{:>8s}{:>10s}'.format('X[:,0]', 'X[:, 1]', 'y'))\n",
    "print('-'*26)\n",
    "for i in range(10):\n",
    "    print('{:8.0f}{:8.0f}{:10.0f}'.format(X[i, 0], X[i, 1], y[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8febee1",
   "metadata": {},
   "source": [
    "## feature normalization \n",
    "\n",
    "### purpose \n",
    "normalizing the features will make, the J (cost function) converge rapidly "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00fc1464",
   "metadata": {},
   "source": [
    "### normalizing features \n",
    "the following function that we will normalize will have as an input : \n",
    "- `X` : which is the features dataset\n",
    "\n",
    "and as an output : \n",
    "\n",
    "- `X_norm` : the normalized version of the features \n",
    "- `mu` : which is an array of the average of each feature \n",
    "- `sigma` : also an array of the standard deviation of each feature "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d0133973",
   "metadata": {},
   "outputs": [],
   "source": [
    "def  featureNormalize(X):\n",
    "    # we will start by intializing the array and matrices \n",
    "    \n",
    "    X_norm = X.copy()\n",
    "    mu = np.zeros(X.shape[1])\n",
    "    sigma = np.zeros(X.shape[1])\n",
    "    \n",
    "    # for each feature we will calculte the average and standard deviation \n",
    "    # then we will normalize that feature \n",
    "    for curr_feat in range(X.shape[1]): \n",
    "        \n",
    "        mu[curr_feat] = np.mean(X[:,curr_feat])\n",
    "        \n",
    "        sigma[curr_feat] = np.std(X[:,curr_feat])\n",
    "        \n",
    "        X_norm[:, curr_feat] = (X_norm[:, curr_feat] - mu[curr_feat])/ sigma[curr_feat]\n",
    "        \n",
    "    return X_norm, mu, sigma"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9193542",
   "metadata": {},
   "source": [
    "## outcome "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d4080c36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computed mean: [2000.68085106    3.17021277]\n",
      "Computed standard deviation: [7.86202619e+02 7.52842809e-01]\n"
     ]
    }
   ],
   "source": [
    "X_norm, mu, sigma = featureNormalize(X)\n",
    "\n",
    "print('Computed mean:', mu)\n",
    "print('Computed standard deviation:', sigma)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a1f3c49",
   "metadata": {},
   "source": [
    "After the `featureNormalize` function is tested, we now add the intercept term to `X_norm`, more explicitly we will add the column of ones that will make us able to multiply `X` by `theta` without leaving behind the `theta_0` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e4a0e8e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.concatenate([np.ones((m, 1)), X_norm], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "180cb410",
   "metadata": {},
   "source": [
    "## Gradient descent \n",
    "<div class=\"alert alert-block alert-warning\">\n",
    "**Implementation Note:** In the multivariate case, the cost function can\n",
    "also be written in the following vectorized form:\n",
    "\n",
    "$$ J(\\theta) = \\frac{1}{2m}(X\\theta - \\vec{y})^T(X\\theta - \\vec{y}) $$\n",
    "\n",
    "where \n",
    "\n",
    "$$ X = \\begin{pmatrix}\n",
    "          - (x^{(1)})^T - \\\\\n",
    "          - (x^{(2)})^T - \\\\\n",
    "          \\vdots \\\\\n",
    "          - (x^{(m)})^T - \\\\ \\\\\n",
    "        \\end{pmatrix} \\qquad \\mathbf{y} = \\begin{bmatrix} y^{(1)} \\\\ y^{(2)} \\\\ \\vdots \\\\ y^{(m)} \\\\\\end{bmatrix}$$\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "283cae7f",
   "metadata": {},
   "source": [
    "### Computing the cost function J \n",
    "for this we will implement the function `computeCostMulti`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f57e66f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def computeCostMulti(X, y, theta):\n",
    "    m = y.shape[0] # number of training examples\n",
    "    \n",
    "    J = 0\n",
    "    \n",
    "    # calculating the hypothesis the outcome from the hypothesis function \n",
    "    h = X @ theta \n",
    "    \n",
    "    J = (((h - y).T)@((h - y)))/ (2*m)\n",
    "    \n",
    "    return J\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65b13872",
   "metadata": {},
   "source": [
    "## Implementing Gradient descent \n",
    "\n",
    "The objective of linear regression is to minimize the cost function\n",
    "\n",
    "$$ J(\\theta) = \\frac{1}{2m} \\sum_{i=1}^m \\left( h_{\\theta}(x^{(i)}) - y^{(i)}\\right)^2$$\n",
    "\n",
    "where the hypothesis $h_\\theta(x)$ \n",
    "$$ h_\\theta(x) = \\theta^Tx$$\n",
    "\n",
    "Recall that the parameters of your model are the $\\theta_j$ values. These are\n",
    "the values you will adjust to minimize cost $J(\\theta)$. One way to do this is to\n",
    "use the batch gradient descent algorithm. In batch gradient descent, each\n",
    "iteration performs the update\n",
    "\n",
    "$$ \\theta_j = \\theta_j - \\alpha \\frac{1}{m} \\sum_{i=1}^m \\left( h_\\theta(x^{(i)}) - y^{(i)}\\right)x_j^{(i)} \\qquad \\text{simultaneously update } \\theta_j \\text{ for all } j$$\n",
    "\n",
    "With each step of gradient descent, your parameters $\\theta_j$ come closer to the optimal values that will achieve the lowest cost J($\\theta$)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e54f4117",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradientDescentMulti(X, y, theta, alpha, num_iters):\n",
    "\n",
    "    m = y.shape[0]  # number of training examples\n",
    "    \n",
    "    theta_copy = theta.copy()\n",
    "    \n",
    "    n_plus = X.shape[1]\n",
    "    \n",
    "    J_history = []\n",
    "    \n",
    "    for i in range(num_iters):\n",
    "        h = X @ theta - y\n",
    "        for cur_feat in range(n_plus): \n",
    "            theta_copy[cur_feat] = theta[cur_feat] - (alpha/m)*np.dot(h, X[:,cur_feat]) \n",
    "        theta = theta_copy.copy()   \n",
    "        J_history.append(computeCost(X, y, theta))\n",
    "    \n",
    "    return theta, J_history"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d05b410",
   "metadata": {},
   "source": [
    "### Testing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "75ede74e",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'computeCost' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_15824/609525394.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m# init theta and run gradient descent\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mtheta\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[0mtheta\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mJ_history\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgradientDescentMulti\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtheta\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_iters\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'theta computed from gradient descent: {:s}'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtheta\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_15824/2679570324.py\u001b[0m in \u001b[0;36mgradientDescentMulti\u001b[1;34m(X, y, theta, alpha, num_iters)\u001b[0m\n\u001b[0;32m     14\u001b[0m             \u001b[0mtheta_copy\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcur_feat\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtheta\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcur_feat\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m-\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0malpha\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mm\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mh\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcur_feat\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m         \u001b[0mtheta\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtheta_copy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m         \u001b[0mJ_history\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcomputeCost\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtheta\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     17\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mtheta\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mJ_history\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'computeCost' is not defined"
     ]
    }
   ],
   "source": [
    "alpha = 0.1\n",
    "num_iters = 400\n",
    "\n",
    "# init theta and run gradient descent\n",
    "theta = np.zeros(3)\n",
    "theta, J_history = gradientDescentMulti(X, y, theta, alpha, num_iters)\n",
    "\n",
    "print('theta computed from gradient descent: {:s}'.format(str(theta)))\n",
    "normalized_x = [1, ((1650-mu[0])/sigma[0]), ((3-mu[1])/sigma[1])]\n",
    "price = np.dot(normalized_x, theta) \n",
    "\n",
    "print('Predicted price of a 1650 sq-ft, 3 br house (using gradient descent): ${:.0f}'.format(price))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "995abe26",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
