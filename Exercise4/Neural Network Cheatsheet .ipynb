{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "99a38328",
   "metadata": {},
   "source": [
    "# Data and Libraries loading  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe87974f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# used for manipulating directory paths\n",
    "import os\n",
    "\n",
    "# Scientific and vector computation for python\n",
    "import numpy as np\n",
    "\n",
    "# Plotting library\n",
    "from matplotlib import pyplot\n",
    "\n",
    "# Optimization module in scipy\n",
    "from scipy import optimize\n",
    "\n",
    "# will be used to load MATLAB mat datafile format\n",
    "from scipy.io import loadmat\n",
    "\n",
    "# library written for this exercise providing additional functions for assignment submission, and others\n",
    "import utils"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4a2d879",
   "metadata": {},
   "source": [
    "### Loading the data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b38ce417",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  training data stored in arrays X, y\n",
    "data = loadmat(os.path.join('Data', 'ex4data1.mat'))\n",
    "X, y = data['X'], data['y'].ravel()\n",
    "\n",
    "# set the zero digit to 0, rather than its mapped 10 in this dataset\n",
    "# This is an artifact due to the fact that this dataset was used in \n",
    "# MATLAB where there is no index 0\n",
    "y[y == 10] = 0\n",
    "\n",
    "# Number of training examples\n",
    "m = y.size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59cfb911",
   "metadata": {},
   "source": [
    "## Neural network\n",
    "\n",
    "### Model representation\n",
    "![](Figures/neural_network.png)\n",
    "\n",
    "### Setting up the neural network "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33d3ac43",
   "metadata": {},
   "source": [
    "##### input layer size : \n",
    "The first layer (input) consists of $400$ neurons(without counting the bias). \n",
    "Because given a $\\mathbb{R}^{20*20}$ by image, we have then a matrix of $\\mathbb{R}^{20*20}$ which we can then turn into a vector of dimension $\\mathbb{R}^{400}$.\n",
    "\n",
    "In such way every image in our dataset X can be turned into a vector of dimension $\\mathbb{R}^{400}$. \n",
    "Finally our dataset of $5000$ images will be a $\\mathbb{R}^{5000*400}$ matrix. \n",
    "\n",
    "`m` : being the number of training examples \n",
    "every row of the matrix `X` : represents a training example \n",
    "\n",
    "\n",
    "$$ X = \\begin{bmatrix} - \\left(x^{(1)} \\right)^T - \\\\\n",
    "- \\left(x^{(2)} \\right)^T - \\\\\n",
    "\\vdots \\\\\n",
    "- \\left(x^{(m)} \\right)^T - \\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "\n",
    "We will have a matrix later that represents the y expected outcome "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9c692ea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_layer_size  = 400  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2752431f",
   "metadata": {},
   "source": [
    "### Second layer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "73c35aef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 25 hidden units (neurons)\n",
    "hidden_layer_size = 25  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cac45a0",
   "metadata": {},
   "source": [
    "### Third layer (input layer): "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7cbf8a69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10 labels, from 0 to 9 (this is )\n",
    "num_labels = 10          "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "966b942e",
   "metadata": {},
   "source": [
    "### Weights Loading "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b7a49a7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the weights into variables Theta1 and Theta2\n",
    "weights = loadmat(os.path.join('Data', 'ex4weights.mat'))\n",
    "\n",
    "# Theta1 has size 25 x 401\n",
    "# Theta2 has size 10 x 26\n",
    "Theta1, Theta2 = weights['Theta1'], weights['Theta2']\n",
    "\n",
    "# swap first and last columns of Theta2, due to legacy from MATLAB indexing, \n",
    "# since the weight file ex3weights.mat was saved based on MATLAB indexing\n",
    "Theta2 = np.roll(Theta2, 1, axis=0)\n",
    "\n",
    "# Unroll parameters \n",
    "nn_params = np.concatenate([Theta1.ravel(), Theta2.ravel()])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da387c31",
   "metadata": {},
   "source": [
    "# Implementing the cost function "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "716179b7",
   "metadata": {},
   "source": [
    "#### Without regularization : \n",
    "\n",
    "$$ J(\\theta) = \\frac{1}{m} \\sum_{i=1}^{m}\\sum_{k=1}^{K} \\left[ - y_k^{(i)} \\log \\left( \\left( h_\\theta \\left( x^{(i)} \\right) \\right)_k \\right) - \\left( 1 - y_k^{(i)} \\right) \\log \\left( 1 - \\left( h_\\theta \\left( x^{(i)} \\right) \\right)_k \\right) \\right]$$\n",
    "\n",
    "\n",
    "##### With regularization : \n",
    "\n",
    "$$ J(\\theta) = \\frac{1}{m} \\sum_{i=1}^{m}\\sum_{k=1}^{K} \\left[ - y_k^{(i)} \\log \\left( \\left( h_\\theta \\left( x^{(i)} \\right) \\right)_k \\right) - \\left( 1 - y_k^{(i)} \\right) \\log \\left( 1 - \\left( h_\\theta \\left( x^{(i)} \\right) \\right)_k \\right) \\right] + \\frac{\\lambda}{2 m} \\left[ \\sum_{j=1}^{25} \\sum_{k=1}^{400} \\left( \\Theta_{j,k}^{(1)} \\right)^2 + \\sum_{j=1}^{10} \\sum_{k=1}^{25} \\left( \\Theta_{j,k}^{(2)} \\right)^2 \\right] $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce58e6bf",
   "metadata": {},
   "source": [
    "## Feedforward Propagation \n",
    "****\n",
    "To implement the cost function we will have first to implement the Feedforward propagation function \n",
    "\n",
    "Given a dataset $X$, parameters $\\theta _{1}$ and $ \\theta _{2}$. We will be able to compute \n",
    "$h(x)= a_{3}$ for the a whole dataset or for just one training example "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7c40437b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def feedForwardProp(Theta1, Theta2 , X): \n",
    "    if X.ndim == 1:\n",
    "        X = X[None]  # promote to 2-dimensions\n",
    "    \n",
    "    # useful variables\n",
    "    m = X.shape[0]\n",
    "    num_labels = Theta2.shape[0]\n",
    "    \n",
    "    # We are going to add a column of one's to X for bias\n",
    "    X = np.concatenate([np.ones((m, 1)), X], axis=1)\n",
    "    \n",
    "    z_2 = Theta1 @  (X.T)\n",
    "    a_2 = utils.sigmoid(z_2)\n",
    "    \n",
    "    # Again here are going to add a column of one's to X for bias \n",
    "    a_2 = np.concatenate([np.ones((1, m)), a_2], axis=0)\n",
    "    \n",
    "    z_3 = Theta2 @ a_2\n",
    "    a_3 = utils.sigmoid(z_3)\n",
    "    \n",
    "    return (a_3,z_3,a_2,z_2,X)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bcece9b",
   "metadata": {},
   "source": [
    "## Cost function and Backpropagation  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8ef83140",
   "metadata": {},
   "outputs": [],
   "source": [
    "def nnCostFunction(nn_params,\n",
    "                   input_layer_size,\n",
    "                   hidden_layer_size,\n",
    "                   num_labels,\n",
    "                   X, y, lambda_=0.0):\n",
    "    Theta1 = np.reshape(nn_params[:hidden_layer_size * (input_layer_size + 1)],\n",
    "                        (hidden_layer_size, (input_layer_size + 1)))\n",
    "\n",
    "    Theta2 = np.reshape(nn_params[(hidden_layer_size * (input_layer_size + 1)):],\n",
    "                        (num_labels, (hidden_layer_size + 1)))\n",
    "    \n",
    "    \n",
    "    # Calculating cost function - non regularized  \n",
    "    \n",
    "    m = y.size\n",
    "    \n",
    "    # we will transform the y vector which contains the expected outputs from the \n",
    "    # training examples in a matrix of dimensions K x m \n",
    "    # each column in this matrix represents expected outputs for each unit from layer 3\n",
    "    \n",
    "    y_transformed = np.zeros((m,num_labels))\n",
    "    \n",
    "    for idx in range(m): \n",
    "        y_transformed[idx,y[idx]] = 1 \n",
    "    \n",
    "    \n",
    "    '''Cost function unregularized vectorized version'''\n",
    "    a_3 = feedForwardProp(Theta1, Theta2, X)[0]\n",
    "    \n",
    "    # For calculating the cost function J in this case without using a for loop \n",
    "    # we will use a pair-wise matrix multiplication \n",
    "    J = np.sum((-(y_transformed.T)*np.log(a_3) - (1-y_transformed.T)*np.log(1-a_3))) / m \n",
    "    \n",
    "    \n",
    "    '''Cost function regularized vectorized version'''\n",
    "    sum_1 = np.sum(Theta1[:,1:]*Theta1[:,1:])\n",
    "    sum_2 = np.sum(Theta2[:,1:]*Theta2[:,1:])\n",
    "    \n",
    "    final_sum = sum_1 + sum_2 \n",
    "    J += ((final_sum*(lambda_))/(2*m))\n",
    "        \n",
    "    \n",
    "    \n",
    "    \n",
    "    '''Calculating the gradient using backpropagation'''\n",
    "    \n",
    "    delta_1 = 0\n",
    "    delta_2 = 0\n",
    "        \n",
    "    for t in range(m):\n",
    "        curr_y = np.array(y_transformed[t, :])\n",
    "        a_3 = feedForwardProp(Theta1, Theta2, X[t,:])[0]\n",
    "        layer3_error = a_3.T - curr_y\n",
    "        layer3_error = layer3_error.T\n",
    "        \n",
    "        \n",
    "        a_2 = feedForwardProp(Theta1, Theta2, X[t,:])[2]\n",
    "        a_2_term = np.multiply(a_2 ,(1-a_2))\n",
    "    \n",
    "        layer2_error = ((Theta2.T)@ layer3_error) * a_2_term\n",
    "        \n",
    "        \n",
    "        delta_2 += layer3_error@ (a_2.T) \n",
    "        \n",
    "        a_1 = feedForwardProp(Theta1, Theta2, X[t,:])[4]\n",
    "        layer2_error = layer2_error[1:]\n",
    "        delta_1 += layer2_error@ (a_1)\n",
    "        \n",
    "        \n",
    "    \n",
    "    Theta1_grad = np.zeros(Theta1.shape)\n",
    "    Theta2_grad = np.zeros(Theta2.shape)\n",
    "    \n",
    "    Theta1_grad[:,0] = delta_1[:,0]/m \n",
    "    Theta2_grad[:,0] = delta_2[:,0]/m \n",
    "    \n",
    "    Theta1_grad[:,1:] = delta_1[:,1:]/m + (lambda_/m) * Theta1[:,1:]\n",
    "    Theta2_grad[:,1:] = delta_2[:,1:]/m + (lambda_/m) * Theta2[:,1:]\n",
    "    \n",
    "    \n",
    "    grad = np.concatenate([Theta1_grad.ravel(), Theta2_grad.ravel()])\n",
    "\n",
    "    return J, grad\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee16f827",
   "metadata": {},
   "source": [
    "# Training the neural network \n",
    "\n",
    "\n",
    "## Random initialization\n",
    "When training neural networks, it is important to randomly initialize the parameters for symmetry breaking. One effective strategy for random initialization is to randomly select values for $\\Theta^{(l)}$ uniformly in the range $[-\\epsilon_{init}, \\epsilon_{init}]$. You should use $\\epsilon_{init} = 0.12$. This range of values ensures that the parameters are kept small and makes the learning more efficient.\n",
    "\n",
    "<div class=\"alert alert-box alert-warning\">\n",
    "One effective strategy for choosing $\\epsilon_{init}$ is to base it on the number of units in the network. A good choice of $\\epsilon_{init}$ is $\\epsilon_{init} = \\frac{\\sqrt{6}}{\\sqrt{L_{in} + L_{out}}}$ where $L_{in} = s_l$ and $L_{out} = s_{l+1}$ are the number of units in the layers adjacent to $\\Theta^{l}$.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cb2bca1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def randInitializeWeights(L_in, L_out, epsilon_init=0.12): \n",
    "    W = np.random.rand(L_out, 1 + L_in) * 2 * epsilon_init - epsilon_init\n",
    "\n",
    "    return W"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a95fa7f",
   "metadata": {},
   "source": [
    "## Learning the parameters "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20694737",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  After you have completed the assignment, change the maxiter to a larger\n",
    "#  value to see how more training helps.\n",
    "options= {'maxiter': 100}\n",
    "\n",
    "#  You should also try different values of lambda\n",
    "lambda_ = 1\n",
    "\n",
    "# Create \"short hand\" for the cost function to be minimized\n",
    "costFunction = lambda p: nnCostFunction(p, input_layer_size,\n",
    "                                        hidden_layer_size,\n",
    "                                        num_labels, X, y, lambda_)\n",
    "\n",
    "# Now, costFunction is a function that takes in only one argument\n",
    "# (the neural network parameters)\n",
    "res = optimize.minimize(costFunction,\n",
    "                        initial_nn_params,\n",
    "                        jac=True,\n",
    "                        method='TNC',\n",
    "                        options=options)\n",
    "\n",
    "# get the solution of the optimization\n",
    "nn_params = res.x\n",
    "        \n",
    "# Obtain Theta1 and Theta2 back from nn_params\n",
    "Theta1 = np.reshape(nn_params[:hidden_layer_size * (input_layer_size + 1)],\n",
    "                    (hidden_layer_size, (input_layer_size + 1)))\n",
    "\n",
    "Theta2 = np.reshape(nn_params[(hidden_layer_size * (input_layer_size + 1)):],\n",
    "                    (num_labels, (hidden_layer_size + 1)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
