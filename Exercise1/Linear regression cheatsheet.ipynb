{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "093e897f",
   "metadata": {},
   "source": [
    "# Introduction \n",
    "This is a summary(or maybe a cheatsheet) from week 2\n",
    "\n",
    "**we will start by importing some libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "021ccd6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# used for manipulating directory paths\n",
    "import os\n",
    "\n",
    "# Scientific and vector computation for python\n",
    "import numpy as np\n",
    "\n",
    "# Plotting library\n",
    "from matplotlib import pyplot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f974cc6",
   "metadata": {},
   "source": [
    "# Linear regression with mutiple variables "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "118420a1",
   "metadata": {},
   "source": [
    "## Loading data with mutiple features \n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7080b105",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.loadtxt(os.path.join('Data', 'ex1data2.txt'), delimiter=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d08a128c",
   "metadata": {},
   "source": [
    "We will then put the features in X(which are in this case: the size of the house and the number of bedrooms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "354daa90",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data[:, :2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eae6c2a",
   "metadata": {},
   "source": [
    "and then the expected outcome y (which is in this case : the price of the house in y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6a980cb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = data[:, 2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4864c8f8",
   "metadata": {},
   "source": [
    "We will see now what the data looks like "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "71daa4aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  X[:,0] X[:, 1]         y\n",
      "--------------------------\n",
      "    2104       3    399900\n",
      "    1600       3    329900\n",
      "    2400       3    369000\n",
      "    1416       2    232000\n",
      "    3000       4    539900\n",
      "    1985       4    299900\n",
      "    1534       3    314900\n",
      "    1427       3    198999\n",
      "    1380       3    212000\n",
      "    1494       3    242500\n"
     ]
    }
   ],
   "source": [
    "# m represents the number of training examples \n",
    "m = y.size\n",
    "\n",
    "# print out some data points\n",
    "print('{:>8s}{:>8s}{:>10s}'.format('X[:,0]', 'X[:, 1]', 'y'))\n",
    "print('-'*26)\n",
    "for i in range(10):\n",
    "    print('{:8.0f}{:8.0f}{:10.0f}'.format(X[i, 0], X[i, 1], y[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4737fd2",
   "metadata": {},
   "source": [
    "## feature normalization \n",
    "***\n",
    "\n",
    "### purpose \n",
    "normalizing the features will make, the J (cost function) converge rapidly "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72aa1b4d",
   "metadata": {},
   "source": [
    "### normalizing features \n",
    "the following function that we will normalize will have as an input : \n",
    "- `X` : which is the features dataset\n",
    "\n",
    "<div class=\"alert alert-block alert-danger\">\n",
    "<b>Before using the hypothesis function h :</b> You should always normalize the features \n",
    "</div>\n",
    "\n",
    "and as an output : \n",
    "\n",
    "- `X_norm` : the normalized version of the features \n",
    "- `mu` : which is an array of the average of each feature \n",
    "- `sigma` : also an array of the standard deviation of each feature "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5ed7c9b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def featureNormalize(X):\n",
    "    # we will start by intializing the array and matrices \n",
    "    \n",
    "    X_norm = X.copy()\n",
    "    mu = np.zeros(X.shape[1])\n",
    "    sigma = np.zeros(X.shape[1])\n",
    "    \n",
    "    # for each feature we will calculte the average and standard deviation \n",
    "    # then we will normalize that feature \n",
    "    for curr_feat in range(X.shape[1]): \n",
    "        \n",
    "        mu[curr_feat] = np.mean(X[:,curr_feat])\n",
    "        \n",
    "        sigma[curr_feat] = np.std(X[:,curr_feat])\n",
    "        \n",
    "        X_norm[:, curr_feat] = (X_norm[:, curr_feat] - mu[curr_feat])/ sigma[curr_feat]\n",
    "        \n",
    "    return X_norm, mu, sigma"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36503896",
   "metadata": {},
   "source": [
    "### outcome "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5e9044e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computed mean: [2000.68085106    3.17021277]\n",
      "Computed standard deviation: [7.86202619e+02 7.52842809e-01]\n"
     ]
    }
   ],
   "source": [
    "X_norm, mu, sigma = featureNormalize(X)\n",
    "\n",
    "print('Computed mean:', mu)\n",
    "print('Computed standard deviation:', sigma)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e71a9180",
   "metadata": {},
   "source": [
    "After the `featureNormalize` function is tested, we now add the intercept term to `X_norm`, more explicitly we will add the column of ones that will make us able to multiply `X` by `theta` without leaving behind the `theta_0` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3793fed0",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.concatenate([np.ones((m, 1)), X_norm], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6361945",
   "metadata": {},
   "source": [
    "## Gradient descent \n",
    "***\n",
    "Cost function : \n",
    "\n",
    "$$ J(\\theta) = \\frac{1}{2m} \\sum_{i=1}^m \\left( h_{\\theta}(x^{(i)}) - y^{(i)}\\right)^2$$\n",
    "\n",
    "<div class=\"alert alert-block alert-warning\">\n",
    "In the multivariate case, the cost function can\n",
    "also be written in the following vectorized form:\n",
    "\n",
    "$$ J(\\theta) = \\frac{1}{2m}(X\\theta - \\vec{y})^T(X\\theta - \\vec{y}) $$\n",
    "\n",
    "where \n",
    "\n",
    "$$ X = \\begin{pmatrix}\n",
    "          - (x^{(1)})^T - \\\\\n",
    "          - (x^{(2)})^T - \\\\\n",
    "          \\vdots \\\\\n",
    "          - (x^{(m)})^T - \\\\ \\\\\n",
    "        \\end{pmatrix} \\qquad \\mathbf{y} = \\begin{bmatrix} y^{(1)} \\\\ y^{(2)} \\\\ \\vdots \\\\ y^{(m)} \\\\\\end{bmatrix}$$\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19270a93",
   "metadata": {},
   "source": [
    "### Computing the cost function J \n",
    "for this we will implement the function `computeCostMulti` (Vectorized version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "33733e0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def computeCostMulti(X, y, theta):\n",
    "    m = y.shape[0] # number of training examples\n",
    "    \n",
    "    J = 0\n",
    "    \n",
    "    # calculating the hypothesis the outcome from the hypothesis function \n",
    "    h = X @ theta \n",
    "    \n",
    "    \n",
    "    J = (((h - y).T)@((h - y)))/ (2*m)\n",
    "    \n",
    "    return J\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35eeca47",
   "metadata": {},
   "source": [
    "## Implementing Gradient descent \n",
    "\n",
    "The objective of linear regression is to minimize the cost function\n",
    "\n",
    "$$ J(\\theta) = \\frac{1}{2m} \\sum_{i=1}^m \\left( h_{\\theta}(x^{(i)}) - y^{(i)}\\right)^2$$\n",
    "\n",
    "where the hypothesis $h_\\theta(x)$ \n",
    "$$ h_\\theta(x) = \\theta^Tx$$\n",
    "\n",
    "Recall that the parameters of your model are the $\\theta_j$ values. These are\n",
    "the values you will adjust to minimize cost $J(\\theta)$. One way to do this is to\n",
    "use the batch gradient descent algorithm. In batch gradient descent, each\n",
    "iteration performs the update\n",
    "\n",
    "$$ \\theta_j = \\theta_j - \\alpha \\frac{1}{m} \\sum_{i=1}^m \\left( h_\\theta(x^{(i)}) - y^{(i)}\\right)x_j^{(i)} \\qquad \\text{simultaneously update } \\theta_j \\text{ for all } j$$\n",
    "\n",
    "With each step of gradient descent, your parameters $\\theta_j$ come closer to the optimal values that will achieve the lowest cost J($\\theta$)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e10b2f56",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradientDescentMulti(X, y, theta, alpha, num_iters):\n",
    "\n",
    "    m = y.shape[0]  # number of training examples\n",
    "    \n",
    "    # we keep a copy of the thetas for a simultaneous update of all the thetas \n",
    "    theta_copy = theta.copy()\n",
    "    \n",
    "    #number of features - note that we added a column of one's in the matrix X \n",
    "    n_plus = X.shape[1]\n",
    "    \n",
    "    #useful for checking if the cost function is converging \n",
    "    J_history = []\n",
    "    \n",
    "    for i in range(num_iters):\n",
    "        error = X @ theta - y\n",
    "        \n",
    "        #gradient descent \n",
    "        for cur_feat in range(n_plus): \n",
    "            theta_copy[cur_feat] = theta[cur_feat] - (alpha/m)*np.dot(error, X[:,cur_feat]) \n",
    "            \n",
    "        # simultanous update of all thetas \n",
    "        theta = theta_copy.copy()   \n",
    "        \n",
    "        J_history.append(computeCostMulti(X, y, theta))\n",
    "    \n",
    "    return theta, J_history"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f1d1207",
   "metadata": {},
   "source": [
    "### Testing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7106890b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "theta computed from gradient descent: [340412.65957447 109447.79558639  -6578.3539709 ]\n",
      "Predicted price of a 1650 sq-ft, 3 br house (using gradient descent): $293081\n"
     ]
    }
   ],
   "source": [
    "alpha = 0.1\n",
    "num_iters = 400\n",
    "\n",
    "# init theta and run gradient descent\n",
    "theta = np.zeros(3)\n",
    "theta, J_history = gradientDescentMulti(X, y, theta, alpha, num_iters)\n",
    "\n",
    "print('theta computed from gradient descent: {:s}'.format(str(theta)))\n",
    "normalized_x = [1, ((1650-mu[0])/sigma[0]), ((3-mu[1])/sigma[1])]\n",
    "price = np.dot(normalized_x, theta) \n",
    "\n",
    "print('Predicted price of a 1650 sq-ft, 3 br house (using gradient descent): ${:.0f}'.format(price))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f267987e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
