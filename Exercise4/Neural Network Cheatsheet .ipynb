{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0f02ac70",
   "metadata": {},
   "source": [
    "# Data and Libraries loading  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c791b9df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# used for manipulating directory paths\n",
    "import os\n",
    "\n",
    "# Scientific and vector computation for python\n",
    "import numpy as np\n",
    "\n",
    "# Plotting library\n",
    "from matplotlib import pyplot\n",
    "\n",
    "# Optimization module in scipy\n",
    "from scipy import optimize\n",
    "\n",
    "# will be used to load MATLAB mat datafile format\n",
    "from scipy.io import loadmat\n",
    "\n",
    "# library written for this exercise providing additional functions for assignment submission, and others\n",
    "import utils"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4a2d879",
   "metadata": {},
   "source": [
    "### Loading the data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b38ce417",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  training data stored in arrays X, y\n",
    "data = loadmat(os.path.join('Data', 'ex4data1.mat'))\n",
    "X, y = data['X'], data['y'].ravel()\n",
    "\n",
    "# set the zero digit to 0, rather than its mapped 10 in this dataset\n",
    "# This is an artifact due to the fact that this dataset was used in \n",
    "# MATLAB where there is no index 0\n",
    "y[y == 10] = 0\n",
    "\n",
    "# Number of training examples\n",
    "m = y.size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59cfb911",
   "metadata": {},
   "source": [
    "## Neural network\n",
    "\n",
    "### Model representation\n",
    "![](Figures/neural_network.png)\n",
    "\n",
    "### Setting up the neural network "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33d3ac43",
   "metadata": {},
   "source": [
    "##### input layer size : \n",
    "The first layer (input) consists of $400$ neurons(without counting the bias). \n",
    "Because given a $\\mathbb{R}^{20*20}$ by image, we have then a matrix of $\\mathbb{R}^{20*20}$ which we can then turn into a vector of dimension $\\mathbb{R}^{400}$.\n",
    "\n",
    "In such way every image in our dataset X can be turned into a vector of dimension $\\mathbb{R}^{400}$. \n",
    "Finally our dataset of $5000$ images will be a $\\mathbb{R}^{5000*400}$ matrix. \n",
    "\n",
    "`m` : being the number of training examples \n",
    "every row of the matrix `X` : represents a training example \n",
    "\n",
    "\n",
    "$$ X = \\begin{bmatrix} - \\left(x^{(1)} \\right)^T - \\\\\n",
    "- \\left(x^{(2)} \\right)^T - \\\\\n",
    "\\vdots \\\\\n",
    "- \\left(x^{(m)} \\right)^T - \\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "\n",
    "We will have a matrix later that represents the y expected outcome "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9c692ea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_layer_size  = 400  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2752431f",
   "metadata": {},
   "source": [
    "### Second layer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "73c35aef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 25 hidden units (neurons)\n",
    "hidden_layer_size = 25  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cac45a0",
   "metadata": {},
   "source": [
    "### Third layer (input layer): "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7cbf8a69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10 labels, from 0 to 9 (this is )\n",
    "num_labels = 10          "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "966b942e",
   "metadata": {},
   "source": [
    "### Weights Loading "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b7a49a7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the weights into variables Theta1 and Theta2\n",
    "weights = loadmat(os.path.join('Data', 'ex4weights.mat'))\n",
    "\n",
    "# Theta1 has size 25 x 401\n",
    "# Theta2 has size 10 x 26\n",
    "Theta1, Theta2 = weights['Theta1'], weights['Theta2']\n",
    "\n",
    "# swap first and last columns of Theta2, due to legacy from MATLAB indexing, \n",
    "# since the weight file ex3weights.mat was saved based on MATLAB indexing\n",
    "Theta2 = np.roll(Theta2, 1, axis=0)\n",
    "\n",
    "# Unroll parameters \n",
    "nn_params = np.concatenate([Theta1.ravel(), Theta2.ravel()])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da387c31",
   "metadata": {},
   "source": [
    "### Cost function "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "716179b7",
   "metadata": {},
   "source": [
    "#### Without regularization : \n",
    "\n",
    "$$ J(\\theta) = \\frac{1}{m} \\sum_{i=1}^{m}\\sum_{k=1}^{K} \\left[ - y_k^{(i)} \\log \\left( \\left( h_\\theta \\left( x^{(i)} \\right) \\right)_k \\right) - \\left( 1 - y_k^{(i)} \\right) \\log \\left( 1 - \\left( h_\\theta \\left( x^{(i)} \\right) \\right)_k \\right) \\right]$$\n",
    "\n",
    "\n",
    "##### With regularization : \n",
    "\n",
    "$$ J(\\theta) = \\frac{1}{m} \\sum_{i=1}^{m}\\sum_{k=1}^{K} \\left[ - y_k^{(i)} \\log \\left( \\left( h_\\theta \\left( x^{(i)} \\right) \\right)_k \\right) - \\left( 1 - y_k^{(i)} \\right) \\log \\left( 1 - \\left( h_\\theta \\left( x^{(i)} \\right) \\right)_k \\right) \\right] + \\frac{\\lambda}{2 m} \\left[ \\sum_{j=1}^{25} \\sum_{k=1}^{400} \\left( \\Theta_{j,k}^{(1)} \\right)^2 + \\sum_{j=1}^{10} \\sum_{k=1}^{25} \\left( \\Theta_{j,k}^{(2)} \\right)^2 \\right] $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce58e6bf",
   "metadata": {},
   "source": [
    "#### Feedforward Propagation \n",
    "To implement the cost function we will have first to implement the Feedforward propagation function \n",
    "\n",
    "Given a dataset X, parameters Thetat1 and Theata2. We will be able to compute \n",
    "h(X)=a_3 for the a whole dataset or for just one training example "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7c40437b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def feedForwardProp(Theta1, Theta2 , X): \n",
    "    if X.ndim == 1:\n",
    "        X = X[None]  # promote to 2-dimensions\n",
    "    \n",
    "    # useful variables\n",
    "    m = X.shape[0]\n",
    "    num_labels = Theta2.shape[0]\n",
    "    \n",
    "    X = np.concatenate([np.ones((m, 1)), X], axis=1)\n",
    "    \n",
    "    z_2 = Theta1 @  (X.T)\n",
    "    a_2 = utils.sigmoid(z_2)\n",
    "    \n",
    "    # adding ones \n",
    "    a_2 = np.concatenate([np.ones((1, m)), a_2], axis=0)\n",
    "    \n",
    "    z_3 = Theta2 @ a_2\n",
    "    a_3 = utils.sigmoid(z_3)\n",
    "    \n",
    "    return (a_3,z_3,a_2,z_2,X)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bcece9b",
   "metadata": {},
   "source": [
    "#### cost function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8ef83140",
   "metadata": {},
   "outputs": [],
   "source": [
    "def nnCostFunction(nn_params,\n",
    "                   input_layer_size,\n",
    "                   hidden_layer_size,\n",
    "                   num_labels,\n",
    "                   X, y, lambda_=0.0):\n",
    "    Theta1 = np.reshape(nn_params[:hidden_layer_size * (input_layer_size + 1)],\n",
    "                        (hidden_layer_size, (input_layer_size + 1)))\n",
    "\n",
    "    Theta2 = np.reshape(nn_params[(hidden_layer_size * (input_layer_size + 1)):],\n",
    "                        (num_labels, (hidden_layer_size + 1)))\n",
    "    \n",
    "    \n",
    "    # Calculating cost function - non regularized  \n",
    "    \n",
    "    m = y.size\n",
    "    \n",
    "    # we will transform the y vector which contains the expected outputs from the \n",
    "    # training examples in a matrix of dimensions K x m \n",
    "    # each column in this matrix represents expected outputs for each unit from layer 3\n",
    "    \n",
    "    y_transformed = np.zeros((m,num_labels))\n",
    "    \n",
    "    for idx in range(m): \n",
    "        y_transformed[idx,y[idx]] = 1 \n",
    "    \n",
    "    \n",
    "        J = 0\n",
    "    \n",
    "    for x in range(m): \n",
    "        a_3 = feedForwardProp(Theta1, Theta2, X[x,:])[0]\n",
    "        ''' \n",
    "        This is a matrix implementation of the second sum for in the cost function \n",
    "        The first sum sum is done by this for loop \n",
    "        we have here transformed y vector in a matrix : y_transformed with dimension (m,K)\n",
    "        where each row of the y_transformed matrix represents the expected output \n",
    "        from a single neuron in the last layer\n",
    "        \n",
    "        The purpose of this transformation is that with a single dot product of the vector a_3 and one row \n",
    "        from the transformed_y matrix we will be able to calculte the second sum without a for loop \n",
    "        \n",
    "        i will try later to make this cost function later as a single matrix multiplication (if possible)\n",
    "        '''\n",
    "        J += ((-y_transformed[x, :] @ np.log(a_3)) - ((1-y_transformed[x, :])@(np.log(1-a_3))))\n",
    "        \n",
    "    J = J/m \n",
    "    \n",
    "    \n",
    "    # Calculating cost function - regularized \n",
    "    \n",
    "    sum_1 = 0 \n",
    "\n",
    "    for neuron in range(hidden_layer_size): \n",
    "        sum_1 += np.dot(Theta1[neuron, 1:]   , Theta1[neuron, 1:] )\n",
    "    \n",
    "    \n",
    "    sum_2 = 0 \n",
    "    \n",
    "    for neuron in range(num_labels): \n",
    "        sum_2 += np.dot(Theta2[neuron, 1:]   , Theta2[neuron, 1:] )\n",
    "\n",
    "    final_sum = sum_1 + sum_2 \n",
    "    last_therm = ((final_sum*(lambda_))/(2*m))\n",
    "    J += last_therm\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    # Calculating the gradient \n",
    "    \n",
    "    delta_1 = 0\n",
    "    delta_2 = 0\n",
    "        \n",
    "    for t in range(m):\n",
    "        curr_y = np.array(y_transformed[t, :])\n",
    "        a_3 = feedForwardProp(Theta1, Theta2, X[t,:])[0]\n",
    "        layer3_error = a_3.T - curr_y\n",
    "        layer3_error = layer3_error.T\n",
    "        \n",
    "        \n",
    "        a_2 = feedForwardProp(Theta1, Theta2, X[t,:])[2]\n",
    "        a_2_term = np.multiply(a_2 ,(1-a_2))\n",
    "    \n",
    "        layer2_error = ((Theta2.T)@ layer3_error) * a_2_term\n",
    "        \n",
    "        \n",
    "        \n",
    "        delta_2 += layer3_error@ (a_2.T) \n",
    "        \n",
    "        a_1 = feedForwardProp(Theta1, Theta2, X[t,:])[4]\n",
    "        layer2_error = layer2_error[1:]\n",
    "        delta_1 += layer2_error@ (a_1)\n",
    "        \n",
    "        \n",
    "    \n",
    "    Theta1_grad = np.zeros(Theta1.shape)\n",
    "    Theta2_grad = np.zeros(Theta2.shape)\n",
    "    \n",
    "    Theta1_grad[:,0] = delta_1[:,0]/m \n",
    "    Theta2_grad[:,0] = delta_2[:,0]/m \n",
    "    \n",
    "    Theta1_grad[:,1:] = delta_1[:,1:]/m + (lambda_/m) * Theta1[:,1:]\n",
    "    Theta2_grad[:,1:] = delta_2[:,1:]/m + (lambda_/m) * Theta2[:,1:]\n",
    "    \n",
    "    \n",
    "    grad = np.concatenate([Theta1_grad.ravel(), Theta2_grad.ravel()])\n",
    "\n",
    "    return J, grad\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67d50745",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
